#### Conor Iannetti, Wayne Zheng, Benson Li, Raj Patel

#### CIS 4496: Projects in Data Science

#### 2/17/22

<p align="center">
  <b>Bi-Monthly Progress Report I</b>
</p>

<b>Introduction:</b>

The project we chose is a generative adversarial networks (GANs) related competition called “I’m Something of a Painter Myself.” The scope of the project revolves around the unique stylistic elements of Claude Monet and possibly other renowned artists and attempting to convert images to their style of painting using GANs. GANs consist of a minimum of two neural networks, a generator model and a discriminator model, which compete with each other to achieve higher accuracy. The goal of this competition is to build a GAN that can generate 7,000 to 10,000 Monet-style images at a high level of accuracy. A user will also be able to upload their own images that they want to be replicated into a Monet style. Right now, we are working on converting images to Monet’s style of painting, and if time permits, we hope to expand it to feature stylistic elements of other renowned artists as well as possibly incorporating a natural language element to allow for text to image translation. Below is a diagram of how a GAN works.

![GANs](https://user-images.githubusercontent.com/60633000/154616058-6805d8a2-1743-4ee6-a0ae-70349aff5f61.png)
*Figure 1. GAN Structure and Workflow*
![GANMultipleStyles drawio](https://user-images.githubusercontent.com/60633000/154616234-623c1256-cc09-4f38-8396-dfd87e431cfa.svg)
*Figure 2. Project Goal High-Level View*

<b>Progress:</b>

<ins>Data Preparation:</ins>

The dataset consists of four directories: a Monet directory which contains Monet paintings, a photo directory that contains photos that we need to turn into Monet-style, and two tfrec directories which contain the photos in a TFRecord format. We have not done any major data augmentation as of yet, the most important thing is that all the images are the same size. We might possibly look for more Monet-style images to include in our training set to have our model trained on more examples. We will utilize a large variety of images to test to make sure that the model is able to replicate Monet stylistic elements across all of them. Since we are also planning on incorporating the works of another artist, we will need to gather a dataset consisting of images in that style for training and can utilize sites such as DeviantArt or Pinterest to find them. 

<ins>Methods:</ins>

CycleGAN is the GAN model that we chose to work with because after doing some research, it is the best one for image to image generation and performed the best on the Kaggle leaderboard. The CycleGAN trains the model automatically and it does not need paired examples which is perfect for our project. To conduct this training, the CycleGAN utilizes a cycle consistency loss function to update the model at each step. Cycle consistency loss uses the L1 norm to compare the absolute difference between pixels of an original image to a twice-cycled image that was translated from photo to Monet painting and back into an original photo, testing the performance of both GANs making up the CycleGan. Additionally,  each of the generators and discriminators are evaluated using binary cross entropy loss in comparison to a matrix of 1’s for real images and 0's for fake. Another model that is critical for our project is the Keras model.Keras is a neural network API (application programming interface) that is integrated with TensorFlow to build machine learning models. Keras is essential for training our model to produce Monet-style images. We used TensorFlow to code the generator and discriminator model for the CycleGAN model. The model also has a residual block in which each layer feeds into the next layer and directly into the layers 2-3 hops away. The residual block creates an identity mapping to activations earlier in the network to address the performance degradation problem associated with deep neural networks, and it eliminates the vanishing gradient problem since it will skip certain connections. We incorporated upsampling and downsampling functions in generator models to avoid the deconvolution and checkerboard artifacts that could occur in the images generated by deep neural networks by increasing/decreasing the rate of sampled signal.

<ins>Performance:</ins>

The performance is based on MiFID (Memorization-informed Frechet Inception Distance) and the smaller the distance, the better the generated images are. MiFID is a modification of FID (Frechet Inception Distance) that penalizes models producing images too similar to the training set. It uses the Inception network to extract features from an intermediate layer and then models the data distributions for the chosen features using mean and covariance. As a result, it tells us how well the model can mimic human perception in similarity in images while promoting diversification in the output. These metrics for performance were chosen because they give a mathematical representation of how “good” the Monet-style images are. Individuals are able to make adjustments and resubmit to achieve a lower score. Our performance for the baseline model was 88.48 when we submitted it on Kaggle. It was not great, but it is something that we are okay with because we were not expecting to do extremely well in the first trial and we have plans to improve on it in the next few weeks. When the image is submitted in the nature view, it produces a decent Monet style image, but if we submit a city view image or something from the modern era, it appears blurry. For example, the image on the left below does not appear to be in a Monet style as it looks a little pixelated and has some black spots. Whereas the image on the right which is a natural photo has converted into a Monet-style image quite well. We are not sure why this is the case right now but we will spend time to see what adjustments we need to make to correct that. 


<center>
  
Object  |  Nature
:-------------------------:|:-------------------------:
![4](https://user-images.githubusercontent.com/60633000/154617084-3fb53081-9cf8-41d6-a084-e44bb2b13ec9.jpg)  |  ![10](https://user-images.githubusercontent.com/60633000/154617148-25694a4d-0bd1-440d-839b-2e88c36d9084.jpg)

</center>

<b>Plan:</b>

Currently, we have a model which can produce a Monet-style image using a CycleGan, but we will continue working on it to make it more robust. The plan for the next two weeks is to generate more accurate Monet style images by experimenting with TensorFlow and tuning the current model which includes adding/deleting neural network layers, adjusting the values of the function parameters. We will also look at other notebooks that have done a similar GANs project and see if we can implement some of the techniques that they used for our project. In particular, there is a technique in which we can initialize the FID model before the training step, and each epoch of the training we perform the FID metric on the current checkpoint, then compare whether or not certain changes in the parameters make the model worse in the FID metric, from there we can adjust the model accordingly.  We also need to have the model work better on images that are not a natural landscape. We have planned tasks for us to work on improving our current model and looking at other potential alternatives to see if they give us a better result. A couple of us will look into gathering datasets from other artists which will then be used in phase 2 to produce images in the style of those artists.

<b>Remaining Questions:</b>

There is still a great deal of work to be done to achieve our final goals. We will continue researching TensorFlow and best practices to implement the generator and discriminator to perform optimally. The models run very slowly on our computers so we will need to see if there are ways to make it run faster via Temple’s High-Performance Computing (HPC) clusters. We were able to get around the slow training times by utilizing the TPUs from Kaggle for phase 1. The biggest task we need to work on is improving the model to replicate Monet-style images across all types of images. Since this is the first time we have worked with GANs, we want to spend more time learning about them and successful ways to code them. We know that they can be tricky, especially to train, but they are capable of producing amazing images which makes them absolutely worth it. We are also looking to implement CycleGAN using PyTorch as some submissions with PyTorch implementation have some high-resolution output images.

<b>Expected Results:</b>

After the next two weeks, we hope to have a more robust model that is capable of producing realistic Monet-style images. We will research ways and look at techniques utilized by people who have worked on similar projects and achieved success. We will also become more familiar with TensorFlow and the capabilities it presents by tinkering with various parameters to achieve our results. We hope to utilize the techniques learned over the next two weeks to achieve a lower MiFID score for our GANs project. We hope to be able to feed the model any image and have it produce the same score regardless of the type of image submitted. That is something that is critical for achieving success for this project as the goal is to make it work universally regardless of the type of image submitted. As previously mentioned, we will do more research and try different approaches in order to achieve this goal by the next couple of weeks. Currently, we are 84th out of 93 teams that have submitted their code on Kaggle, we hope to be in the top 50 at least by the next 2 weeks once we make changes to the model.

![image](https://user-images.githubusercontent.com/60633000/154617801-e5c68ba8-3d71-4781-97c2-886d4af52e76.png)

*Figure 3. Kaggle Leaderboard Standing as of 2-17-2022*

<b>Resources:</b>

* [Residual blocks — Building blocks of ResNet | by Sabyasachi Sahoo | Towards Data Science](https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec)
* [1512.03385 Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
* [Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/)
