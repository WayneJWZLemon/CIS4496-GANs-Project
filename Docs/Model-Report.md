Conor Iannetti, Wayne Zheng, Benson Li, Raj Patel

CIS 4496

April 11, 2022

## Model & Performance Report: CycleGAN

### Brief Summary of our Project

The data science project we chose to work on this semester is titled “I am Something of a Painter Myself.” It is a GAN (Generative Adversarial Network) related project conducted on Kaggle to create thousands of Monet-style images. The goal of this project is to produce images in the style of Claude Monet so the discriminator and human inspection would not be able to tell the difference between our generated images and the Monet’s actual paintings. We added to this project and did the same for a handful of different artists including Piscasso, Rembrandt, Sisley, and Degas. There are different types of GANs, we utilized a CycleGAN model to replicate Monet images because it is most well-suited for image to image translation problems. More details of the datasets, models, and performance evaluation are detailed in this report. 

### Which feature engineering techniques were used? Justify your choice and describe the working of the techniques in the context of your project goals.

After our preprocessing techniques, each image is introduced to the model, whether it is a painting or photograph, consisting of a 256x256 image with its colors expressed in the 3-channel RGB format. Since our model is made to process and convert the styles of these unlabeled images, the features consist of the array of pixels within each image and the colors within them. Since each pixel in the image was of equal importance during training to learn the patterns of the painter’s style, it was difficult to discern which, if any, features should take precedence over the others. Therefore, our focus shifted to providing the model with more image data to process overall by increasing the size of our training dataset with the use of data augmentation techniques. These methods are intended to overcome the limited size of our datasets of paintings by creating new images to add to the training set based on the ones we have. Since the objective of our project is transferring the style of the paintings onto the photos, we had to specifically select augmentations that would not dramatically change the color distributions the artists used. This caused us to only select spatial augmentations rather than ones that affect the style, such as changing the contrast, brightness, or saturation. The first of these was done before the onset of training, as a horizontally-flipped (mirror image) duplicate of each painting was added to the dataset using the Python Image Library (PIL) to initially double its size. Once training was initialized, the other augmentations were applied using TensorFlow’s built-in image manipulation functions. Upon each epoch, each training image is randomly rotated 90, 180, or 270 degrees, randomly flipped horizontally or vertically, and has a random 128x128 segment be cropped and resized to be used as input. Examples of these modified images are shown below.

![image](https://user-images.githubusercontent.com/60633000/162845758-783f9434-ec13-46a4-bd02-5fb8a576d248.png)

These augmentations allow each image in our dataset to be converted into many similar ones, preventing overfitting by allowing the model to be trained on more varied and diverse input data while retaining the stylistic qualities of the artists’ work.

### Which modeling algorithms were used? Justify your choice and describe the working of the algorithms in the context of your project goals.

The modeling algorithm being used in our project was the CycleGAN (Cycle Generative Adversarial Network). The CycleGAN was picked for our project because it specializes in image translation, the process of converting one image to another type of image, which is what the main goal of our project is. The CycleGAN is a type of generative adversarial network that can perform image to image style transfers without paired examples. There are two domains to keep in mind when working with the CycleGAN: the source domain and the target domain. The source domain in the context of the competition would be the non-painting images and the target domain would be the paintings by the artists. Ordinarily, a GAN would only have one generator and one discriminator, however, the CycleGAN has two generators and two discriminators. One generator is tasked with being able to take images from one domain, this would be the input and output of those images in the style of the images in the second domain. The second generator would perform this same task but with the domains switched. The discriminator models would then be used to see if they can distinguish between the generated images of a certain style and the actual images of that certain style. In other words, one discriminator should be able to distinguish between a real Monet painting and a generated Monet painting and the other discriminator should be able to distinguish an image from the non-painting image dataset and a generated image in the style of the non-painting image dataset. Another addition that separates the CycleGAN from the basic GAN is the requirement of cycle consistency. The basic idea behind cycle consistency is that the output image of the first generator can be used as the input image of the second generator and the output image of the second generator should match the input image of the first generator. The reverse of this statement should also be true for cycle consistency to be met. To ensure that cycle consistency is met, an additional loss function should be added to measure the generated output of the second generator and the input image of the first generator, and vice-versa. 

In addition, we adapted a learning rate scheduler to our models, in which we started training the network with a large learning rate and then gradually decreased the learning rate, this decay of learning rate is observed to help with both generalization and optimization. As mentioned in the original CycleGAN paper by Zhu et al, the researchers set their model learning rate to 0.0002 for the first 100 epochs and then linearly decreased the learning rate to 0 over the next 100 epochs. We did not follow this approach in phase #1 of the project development cycle, because at the time the amount of data we had was not sufficient to worth testing out the decay rate scheduler. During phase #2, we were able to gather more paintings from sources available online, with the increase in our training data and the learning rate scheduler method, we did see a huge improvement overall.

![Phase3HighLevelView](https://user-images.githubusercontent.com/60633000/162845929-b94b2237-3549-443a-97df-307173a124cd.jpg)

*A graphic to illustrate how the CycleGAN functions*

Another factor that swayed us to choose the CycleGAN is that the highest-performing notebooks on the Kaggle leaderboard used the CycleGAN for their final model. There are not many other models that specialize in the task of image translation but the ones available such as pix2pix or the basic GAN were not suitable for this task. The pix2pix model, which is a type of ConditionalGAN, was not suitable for our task because that model requires paired training data while the CycleGAN does not. The basic GAN can perform the task of style conversion but after looking at the performance of the Kaggle notebooks that did do this, we discovered that the performance using this model was very poor.

### Which metrics are used to evaluate the performance of chosen machine learning model(s)? How would you justify the choice of these metrics?

The main metric used to evaluate the performance of our CycleGAN and GANS, in general, is the MiFID score. MiFID stands for Memorization-informed Frechet Inception Distance score and it is a modified version of the FID (Frechet Inception Distance) score. The goal of a GAN is to have its generators create outputs that will pass through the discriminator mistaken for real data. The FID score penalizes models who produce images that are too similar to the training set. It uses the inception network to extract features from an intermediate layer and then models the data distributions for the chosen features using mean and covariance. According to a paper written by Ali Borji, a Postdoctoral Scholar at the University of Southern California, the FID score is widely adopted because of its consistency with human inspection and sensitivity to small changes in the images. Small changes, in this case, would include things like slight blurring in the images or small artifacts in the images. A major drawback of the FID score, according to Borji, is its high bias. Typically, the sample size to calculate the FID score should be very large, Borji states it should be above 50k images, otherwise, there is a risk of overestimation of the actual FID score (Borji, 2021).

The formula below tells us how well the model mimics human perception in similarity while promoting diversification in output. The formula for the MIFID score is: MiFID(Sg, St) = Mτ (Sg, St) · FID(Sg, St).

![image](https://user-images.githubusercontent.com/60633000/162846291-9ad9197b-254e-4687-9b6d-fb083b828a4b.png)

MiFID differs from the original FID formula through the presence of a memorization distance value. Memorization distance consists of the minimum cosine distance of all training images averaged over all generated images, which then lowers the FID output further if it is below a certain threshold. This score takes into account everything that the FID score does and also adds a penalty for models that simply memorize the training set. This performance metric is used on Kaggle to determine the leaderboard for our competition.

Utilizing the MiFID score for determining the performance of GANs, at least for this competition, has both pros and cons. Some pros are that the score is calculated automatically through Kaggle so we do not need to create a function to measure the performance of our model. Another positive is that if your model produces high-quality images, you will get a relatively low MiFID score which makes sense. This sometimes does not go both ways though. For example, if you produce a much better quality image, you will get a lower MiFID score than before. However, if your MiFID score gets a lot lower, it does not necessarily mean that your image quality improved.  A con of this metric is that it is impacted by other factors that will usually result in a lower MiFID score without attaining the desired result, such as simply having more data. Sometimes the score and image quality do not go hand in hand which might make some focus on ways to get a lower MiFID score without trying to improve their image quality. 

Aside from using the FID and MiFID scores to determine if the generated images are sufficient, we would also need to perform manual inspection of the images. In his paper on the various GAN evaluation metrics, Borji states that manually inspecting the generated images is another popular method for evaluating the quality of generated images. Manually inspecting the generated image dataset is a good practice since there are generated images that do not meet the quality desired, even though the discriminator deems the image as passing.

![image](https://user-images.githubusercontent.com/60633000/162846364-f2238f03-9d65-4143-863d-4634cc8b02c8.png)

*An example of a generated image that manual inspection can easily determine is not of sufficient quality.*

Unfortunately, this method comes with a host of issues that Borji mentions in the paper. One main issue being that the person reviewing the image will have biases and incorporate that into their final analysis. Additionally, since none of our members are intimately familiar with an artist’s style means that there could be images that do not match an artist’s style that we deem as passing but that in reality do not. One of the biggest issues with just using manual inspection is that we cannot review every single image in the generated dataset. There are thousands of images in our generated dataset, making it nearly impossible to review every single image for quality assessment. Having good evaluation measures is very important when it comes to measuring the output of a GAN since it can be used for ranking generative models and also diagnosing the errors associated with a GAN. This is especially important when humans are not trained to recognize the quality of samples such as medical images or, in the context of this project, paintings.

### Provide evaluations of your machine learning model(s) using the defined performance metrics.

Using the MIFID score to evaluate our model’s performance, we are able to monitor our progress from one phase to another as well as notice the impacts the size of datasets has on the score. For example, at the end of phase 1, our model was producing images like this (shown below) which resulted in a MiFID score of 80.4.

![image](https://user-images.githubusercontent.com/60633000/162846433-b70a2d4a-afb4-44b4-a21f-da1d3c1cbe9d.png)

As you can see, these images look grainy and very pixelated so the evaluation metric being high tells us that our image quality is poor and not being passed through the discriminator with high accuracy. We would expect to receive a poor MiFID score based on these images so the evaluation metric does its job here. Compare this with phase 2 where our MiFID score was 42.3 and our model was producing Monet’s like this (shown below).

![image](https://user-images.githubusercontent.com/60633000/162846478-55dee52e-a671-498a-878c-4e7b0abe354d.png)

In this case, these images actually look like a painting and the much lower MiFID score tells us that not only is the image quality better but more of them are being passed through the discriminator with higher accuracy. We would expect a low MiFID score here based on the images our model produced looking more like a painting with brushstrokes and different shades of color which Monet would paint with. We were happy that our MiFID score decreased significantly and even more satisfied looking at the resulting images because they resemble Monet’s paintings very well and the improvements we made with the model had a big impact. With our most recent submission on the Kaggle competition, we are sitting in 30th out of 105 teams and our model produces images like those seen above. The team in first place has a MiFId score of 35.16 and ours is at 42.35, so we were able to manage to get pretty close to the leader throughout this semester.

Looking at how the evaluation metric is affected by other factors such as the size of the training set, we can look at the table below. We can see that by using the datasets with the most number of examples, the model spit out a FID score way lower than using the original Kaggle dataset. For example, using the original 300 Monet paintings for both datasets we got an FID score of 124.9. Then once we used the 1193 Monet dataset from Berkeley, our score went down to 61.23 without making any adjustments to the model. In an ideal world, the MiFID score would mainly be dependent on the quality of images produced and not impacted as much by the dataset size. Overall though, this metric for evaluating our model does suffice but it is important to manually inspect the images as well and not just be reliant on the FID score and its variants.

![image](https://user-images.githubusercontent.com/60633000/162846553-66f5d30e-c4d7-4581-9cc6-0cbe5631b085.png)

### References:

* Brownlee, J. (2020, September 1). How to develop a cyclegan for image-to-image			translation with keras. Machine Learning Mastery. Retrieved March 28, 2022,		from https://machinelearningmastery.com/cyclegan-tutorial-with-keras/

* Borji, Ali. "Pros and cons of GAN evaluation measures: New developments." Computer 
Vision and Image Understanding 215 (2022): 103329.

* Zhu, Jun-Yan, et al. “Unpaired Image-To-Image Translation Using Cycle-Consistent		Adversarial Networks.” ArXiv.org, 2017, arxiv.org/abs/1703.10593.




